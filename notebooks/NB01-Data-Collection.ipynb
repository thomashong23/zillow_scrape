{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Zillow data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                         # for sending HTTP requestss\n",
    "from scrapy import Selector             # for parsing HTML content\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Makin basic page fetching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_page(url, delay=1):\n",
    "  # add headers to avoid 403 error\n",
    "  headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "  }\n",
    "  #allow site to load\n",
    "  time.sleep(delay)\n",
    "  try:\n",
    "    response = requests.get(url, timeout=None, headers= headers) \n",
    "  except Exception as e:\n",
    "    print('Error', e)\n",
    "    pass\n",
    "  if not response.ok:\n",
    "    print(\"Something went wrong\", response.status_code)\n",
    "    pass\n",
    "  html = response.content\n",
    "  #return the selector which contaisn the html of the page\n",
    "  return Selector(text=html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get zips from zipcodes.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_zips (sel):\n",
    "  \"\"\"\n",
    "  Returns a list of all the zip codes by \n",
    "  scraping the zipcode.org site given the url in form\n",
    "  'https://zipcode.org/city/{state}/{city}'\n",
    "  \"\"\"\n",
    "  #get the container which holds the list of zip codes\n",
    "  block = sel.css('div.HTML_Block')[5]\n",
    "  #get all zip codes in that container by getting the href attribute since the format is cleaner\n",
    "  zip_list = block.css('a::attr(href)').getall()\n",
    "  #remove the \"/\" from the beginning of each zip code href\n",
    "  adjusted_zip_list = [x[1:] for x in zip_list]\n",
    " \n",
    "  return adjusted_zip_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Zillow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_by_script(full_page_selector):\n",
    "  \"\"\"\n",
    "  Inputs a Zillow page Selector and returns a DataFrame of the listings information\n",
    "  by scraping the script tag which contains the JSON data\n",
    "  \"\"\"\n",
    "  #find the script which loads in the page with data\n",
    "  script_text = full_page_selector.css('script#__NEXT_DATA__').get()\n",
    "  #split the script by necessary chars to get the JSON data alone\n",
    "  no_first = script_text.split('>')[1]\n",
    "  no_last = no_first.split('<')[0]\n",
    "  listing_json = json.loads(no_last)\n",
    "  #dive into the JSON to get the listing data\n",
    "  cat_1 = listing_json['props']['pageProps']['searchPageState']['cat1']['searchResults']['listResults']\n",
    "  page_df = pd.DataFrame(cat_1)\n",
    "  return(page_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Zillow pages with the same Zillow parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_fetch_all_pages(url):\n",
    "  \"\"\"\n",
    "  Inputs: url - the relative url of first page of Zillow listing with some input\n",
    "  Outputs: nothing, but saves the data to a json file in the data folder of that page and all subsequent \n",
    "  pages of the listing which would be accessed by hitting the next arrow\n",
    "  \"\"\"\n",
    "  #get the full url of the page\n",
    "  full_url = 'https://www.zillow.com' + url\n",
    "  #get the selector of the page\n",
    "  sel = fetch_page(full_url)\n",
    "  #get the data from the page\n",
    "  wanted_df = fetch_by_script(sel)\n",
    "\n",
    "  #check if the url is longer than 16 characters, if so, then it is not the first page,\n",
    "  #so edit the url to save the file with a more readable name\n",
    "  if len(url) > 16:\n",
    "    edited_url = url[:17] + '_' + url[17:]\n",
    "  else:\n",
    "    edited_url = url\n",
    "\n",
    "  \n",
    "\n",
    "  #if df is empty, no listings in that area, so I don't want to save the file\n",
    "  if wanted_df.empty:\n",
    "    # print(\"empty\")\n",
    "    return\n",
    "  \n",
    "  #i used AI to save the df to a json in my data folder and check to make sure file exists\n",
    "  if wanted_df is not None:\n",
    "    output_dir = '../data/raw/zillow'\n",
    "    if not os.path.exists(output_dir):\n",
    "      os.makedirs(output_dir)\n",
    "    file_path = os.path.join(output_dir, f'zillow_{edited_url.replace(\"/\",\"\")}.json')\n",
    "    wanted_df.to_json(file_path, orient='records', lines=True)   \n",
    "  else:\n",
    "    print(f'No data found for {url}')\n",
    "  # try to get url for the next page if it exists\n",
    "  try:\n",
    "    footer = sel.xpath('//ul[contains(@class, \"PaginationList\")]/li[contains(@class, \"PaginationJumpItem\")]').css('::attr(href)').getall()[1]\n",
    "  except:\n",
    "    #if it doesn't exist, return to break the recursion\n",
    "    return \n",
    "  if footer is None:\n",
    "    return \n",
    "  \n",
    "  #basically, zillow only lets you get 20 pages, and the next arrow would point\n",
    "  #to the 20th page even if it should point to 21st\n",
    "  if footer == url:\n",
    "    return \n",
    "  return rec_fetch_all_pages(footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zip_url = 'https://zipcode.org/city/MA/BOSTON'\n",
    "#get boston zip lise\n",
    "boston_zip_list = get_all_zips(fetch_page(zip_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pages in the boston_zip_list just pull up the entirety of Boston when I send the query, so I will remove them. I think because these Zips don't cover an actual area, but rather they are one building like a PO box or company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that I did this manually, but if recreating, you could run NB01-Data-Census first, and only use the \n",
    "#zip codes which have some population\n",
    "values_to_remove = ['02112', '02117', '02123', '02133', '02196', '02205',\n",
    "          '02212', '02283', '02211', '02217', '02241', '02284',\n",
    "          '02293', '02295', '02297', '02298', '02206', '02204']\n",
    "\n",
    "for value in values_to_remove:\n",
    "  if value in boston_zip_list:\n",
    "    boston_zip_list.remove(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in boston_zip_list:\n",
    "  rec_fetch_all_pages(\"/ma-\" + x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
