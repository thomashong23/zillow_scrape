{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                         # for sending HTTP requests\n",
    "\n",
    "from tqdm.notebook import tqdm, trange  # for progress bars\n",
    "from selenium import webdriver\n",
    "from scrapy import Selector             # for parsing HTML content\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_page(url, delay=1):\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "  }\n",
    "  time.sleep(delay)\n",
    "  try:\n",
    "    response = requests.get(url, timeout=None, headers= headers) \n",
    "  except Exception as e:\n",
    "    print('Error', e)\n",
    "    pass\n",
    "  if not response.ok:\n",
    "    print(\"Something went wrong\", response.status_code)\n",
    "    pass\n",
    "  html = response.content\n",
    "  \n",
    "  return Selector(text=html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_all_listings(page_sel): \n",
    "#   \"\"\"\n",
    "#   Inputs: \n",
    "#     page_sel: a Selector object which contains \n",
    "#       the contents of the Zillow website of houses\n",
    "#   Outputs:\n",
    "#     rental_list: a list of listings contained within page_sel\n",
    "#   \"\"\"\n",
    "#   objects = page_sel.xpath('//div[contains(@class, \"StyledPropertyCardDataWrapper\")]')\n",
    "#   return objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide this cell\n",
    "# def get_listing_info(listing_sel):\n",
    "#   \"\"\"\n",
    "#   Inputs:\n",
    "#     listing_sel: a Selector object which contains the content\n",
    "#       of one listing\n",
    "#   Outputs:\n",
    "#     unit_dict: a dictionary which contains price, number of bedrooms, \n",
    "#       number of bathrooms, sqft, postal code\n",
    "#   \"\"\"\n",
    "#   address = listing_sel.css(\"a ::text\").get()\n",
    "#   zip_code = address[-5:]\n",
    "#   price = listing_sel.css(\"div span ::text\").get()[1:]\n",
    "#   bbs = listing_sel.css(\"ul li b ::text\").getall()\n",
    "#   bed = bbs[0]\n",
    "#   bath = bbs[1]\n",
    "#   sqft = bbs[2]\n",
    "#   town = address.split(',')[1]\n",
    "#   address = address.split(',')[0]\n",
    "#   return {'address': address, 'zip_code': zip_code, 'price': price, 'num_bedrooms': bed, \n",
    "#           'num_bathrooms': bath, 'sqft': sqft, 'town': town}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# objects = extract_all_listings(sel)\n",
    "# # print(sel)\n",
    "# info_list = [get_listing_info(x) for x in objects]\n",
    "# info_df = pd.DataFrame(info_list)\n",
    "# display(info_df)\n",
    "# # display(info_df.info)\n",
    "# display(sel.css('div#grid-search-results').get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# footer = sel.xpath('//ul[contains(@class, \"PaginationList\")]/li[contains(@class, \"PaginationJumpItem\")]').css('::attr(href)')[1]\n",
    "# print(footer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# footer_list = [get_listing_info(foot) for foot in footer]\n",
    "# display(footer_list)\n",
    "# print(len(footer_list))\n",
    "# print(footer_list[30])\n",
    "# df = pd.DataFrame(footer_list)\n",
    "# display(df)\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recursive_scrape(url, list_so_far):\n",
    "#   if len(list_so_far) > 30:\n",
    "#     return list_so_far\n",
    "#   if url[0] != '/':\n",
    "#     return list_so_far\n",
    "#   url_modified = \"https://www.zillow.com\" + url\n",
    "#   sel = fetch_page(url_modified)\n",
    "#   objects = extract_all_listings(sel)\n",
    "#   info_list = [get_listing_info(x) for x in objects]\n",
    "#   list_so_far.append(info_list)\n",
    "#   footer = sel.xpath('//ul[contains(@class, \"PaginationList\")]/li[contains(@class, \"PaginationJumpItem\")]').css('::attr(href)').getall()[1]\n",
    "#   return recursive_scrape(footer, list_so_far)\n",
    "# agg_list = recursive_scrape('/boston-ma', [])\n",
    "\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_by_script(full_page_selector):\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  #find the script which loads in the page with data\n",
    "  script_text = full_page_selector.css('script#__NEXT_DATA__').get()\n",
    "  no_first = script_text.split('>')[1]\n",
    "  no_last = no_first.split('<')[0]\n",
    "  listing_json = json.loads(no_last)\n",
    "  cat_1 = listing_json['props']['pageProps']['searchPageState']['cat1']['searchResults']['listResults']\n",
    "  page_df = pd.DataFrame(cat_1)\n",
    "  return(page_df)\n",
    "\n",
    "selector = fetch_page('https://www.zillow.com/boston-ma')\n",
    "# display(fetch_by_script(selector))\n",
    "\n",
    "\n",
    "# print(cat_1)\n",
    "\n",
    "\n",
    "#the zpid is the same as id, so dropping zpid\n",
    "#making a new dataframe with only the columns we want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma.json\n",
      "/boston-ma/2_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma2_p.json\n",
      "/boston-ma/3_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma3_p.json\n",
      "/boston-ma/4_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma4_p.json\n",
      "/boston-ma/5_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma5_p.json\n",
      "/boston-ma/6_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma6_p.json\n",
      "/boston-ma/7_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma7_p.json\n",
      "/boston-ma/8_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma8_p.json\n",
      "/boston-ma/9_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma9_p.json\n",
      "/boston-ma/10_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma10_p.json\n",
      "/boston-ma/11_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma11_p.json\n",
      "/boston-ma/12_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma12_p.json\n",
      "/boston-ma/13_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma13_p.json\n",
      "/boston-ma/14_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma14_p.json\n",
      "/boston-ma/15_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma15_p.json\n",
      "/boston-ma/16_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma16_p.json\n",
      "/boston-ma/17_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma17_p.json\n",
      "/boston-ma/18_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma18_p.json\n",
      "/boston-ma/19_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma19_p.json\n",
      "/boston-ma/20_p/\n",
      "DataFrame saved to ../data/raw/zillow/zillow_boston-ma20_p.json\n",
      "/boston-ma/20_p/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def rec_fetch_all_pages(url):\n",
    "  full_url = 'https://www.zillow.com' + url\n",
    "  sel = fetch_page(full_url)\n",
    "  wanted_df = fetch_by_script(sel)\n",
    "  #i used AI to save the df to a json in my data folder and check to make sure file exists\n",
    "  if wanted_df is not None:\n",
    "    output_dir = '../data/raw/zillow'\n",
    "    if not os.path.exists(output_dir):\n",
    "      os.makedirs(output_dir)\n",
    "    file_path = os.path.join(output_dir, f'zillow_{url.replace(\"/\",\"\")}.json')\n",
    "    wanted_df.to_json(file_path, orient='records', lines=True)\n",
    "    print(f'DataFrame saved to {file_path}')\n",
    "     \n",
    "  else:\n",
    "    print(f'No data found for {url}')\n",
    "  footer = sel.xpath('//ul[contains(@class, \"PaginationList\")]/li[contains(@class, \"PaginationJumpItem\")]').css('::attr(href)').getall()[1]\n",
    "  print(footer)\n",
    "  if footer is None:\n",
    "    return\n",
    "  #basically, zillow only lets you get 20 pages, and the next arrow would point\n",
    "  #to the 20th page even if it should point to 21st\n",
    "  if footer == url:\n",
    "    return\n",
    "  return rec_fetch_all_pages(footer)\n",
    "\n",
    "rec_fetch_all_pages('/boston-ma/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
